# Base settings for all latecrop markers.

# Configuration of the images we want to use.
[images]
# The images to be used for this marker
images = ["s2-agri-weekly", {"s1-grd-sigma0-asc-weekly": ["VV", "VH"]}, {"s1-grd-sigma0-desc-weekly": ["VV", "VH"]}, "s1-coh-weekly"]

# Configuration specific to the marker being calculated.
[marker]
markertype = LATECROP

# strategy to balance the training dataset for the marker. Possible values:
#   * BALANCING_STRATEGY_NONE: don't apply any balancing: 20% of the input samples per class is used for training
#   * BALANCING_STRATEGY_MEDIUM: 80% of input data is used for training, with maximum 10.000 samples per class and minimum 1.000 (samples will be duplicated if needed)
#   * BALANCING_STRATEGY_MEDIUM2: 80% of input data is used for training, with maximum 10.000 samples per class depending on input count + minimum 1.000 (samples will be duplicated if needed)
#   * BALANCING_STRATEGY_UPPER_LIMIT: 80% of input data is used for training, with a maximum of 10.000 samples per class
#   * BALANCING_STRATEGY_PROPORTIONAL_GROUPS: 80% of input data is used for training, but for classes with > 10.000 samples +- only half of those are used  
#   * BALANCING_STRATEGY_EQUAL: for each input class, the same amount of samples is used as training. For classes with few samples, (samples will be duplicated if needed)
balancing_strategy = BALANCING_STRATEGY_MEDIUM2

# add classes that will be ignored for training and won't receive a prediction 
classes_to_ignore = ${classes_to_ignore_default}, MON_CG_MOEILIJK_ZONDER_KLASSIFICATIE, MON_CG_MOEILIJK_ZONDER_KLASSIFICATIE_NS, MON_CG_HEG_RIJ_POEL, MON_CG_GRASSEN_BRAAK_NIETSUB, MON_CG_STAL_GEB, MON_CG_GEEN_HOOFDTEELT, MON_CG_CONTAINERS, MON_CG_OVERK_LOO, IGNORE:PERMANENT_BEDEKKING, IGNORE:EARLY_MAINCROP
# add classes that should be ignored for training, but have to get a prediction
classes_to_ignore_for_train = ${classes_to_ignore_for_train_default}

# classes that should specified as unimportant in the reporting
# Remark: this doesn't influence the training or predicting, these need to be set in 
# the other parameters!
classes_to_ignore_unimportant = ${classes_to_ignore_unimportant_default}, MON_CG_MOEILIJK_ZONDER_KLASSIFICATIE, MON_CG_MOEILIJK_ZONDER_KLASSIFICATIE_NS, MON_CG_STAL_GEB, MON_CG_GRASSEN_BRAAK_NIETSUB, MON_CG_BIETEN, MON_CG_MAIS_OF_MENGTEELT, UNKNOWN, IGNORE:PERMANENT_BEDEKKING
# UNKNOWN

# The aggregation type to use on parcel level. Following options are avalable:
#    - mean: the mean of the pixel values in a parcel
#    - median: the median of the pixel values in a parcel
#    - std: the standard deviation of the pixel values in a parcel
parceldata_aggregations_to_use = mean

[columns]
# Column name of the classes to prepropress to in the refe file
class_refe = MON_OCNT

crop = GWSCOD_N
crop_declared = GWSCOD_N
crop_gt_verified = NATEELT_CTRL_COD
crop_gt_unverified = NATEELT_CTRL_COD_ORIG

[classifier]

# The number of cross-prediction-models to use for this marker.
#
# If cross_pred_models is None (the default) or <= 1, , no cross-prediction models are
# used.
#
# The cross-prediction-models feature can be used to avoid predicting a marker on a
# parcel where the parcel was part of the training dataset.
# When cross_pred_models = 2, the input data is split in 2 (per class). Each 50% of
# the data will be used to train one model, and this model will be used to
# predict the other 50% of the parcels.
cross_pred_models = 4

# The classifier type to use. Currently supported types:
#     * keras_multilayer_perceptron (using keras)
#     * multilayer_perceptron (using sklearn)
#     * nearestneighbour
#     * randomforest
#     * svm
classifier_type = randomforest

# The extension of the file format to save the trained model to
classifier_ext = .hdf5

# For all sklearn based classifiers, any kwargs supported by the classifier chosen can
# be specified as a json parameter, e.g. for randomforest:
# classifier_sklearn_kwargs = {
#         "n_estimators": 200,
#         "max_depth": 35,
#         "min_samples_split": 10
#     }

[preprocess]
#extra_export_columns = CODE_OBJ, LAYER_ID, PRC_ID, VERSIENR
#columns_to_keep = MON_EARLY_LATE, GESP_PM, GRAF_OPP, STAT_BGV, GWSCOD_N

# min amount of parcels that need to be in a specific class or it will get moved into IGNORE_NOT_ENOUGH_SAMPLES
min_parcels_in_class = 100

[postprocess]

# The number of top classes to retain per parcel in the result.
top_classes = 4

# Doubt: if highest probability < 2*second probability  
doubt_proba1_st_2_x_proba2 = False
# Doubt: if prediction == input class and highest probability < pct
doubt_pred_eq_input_proba1_st_pct = 0
# Doubt: if prediction != input class and highest probability < pct
#doubt_pred_ne_input_proba1_st_pct = 80
doubt_pred_ne_input_proba1_st_pct = 50

# Doubt: if prediction != input class and result of `proba_correction_eval` < thresshold
# `proba_correction_eval` should be a valid `pandas.eval` expression.
# A typical application is to increase DOUBT parcels by using a correction factor on the
# probability of the predicted class before applying the DOUBT thresshold.
# E.g. using config `proba_correction_eval = pred1_prob * proba_correction_factor` will
# result in using the probability of the predicted class multiplied with a column
# `proba_correction_factor` to compare this with the DOUBT thresshold. Column
# `proba_correction_factor` should be available when postprocessing, so should be added
# or retained during preprocessing.
proba_correction_eval = pred1_prob * proba_correction_factor