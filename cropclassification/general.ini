# This is a config file with the shared/default settings for all markers. 

# The task section contains information about the task you want to run 
[task]

# The action you want to run:
#   - calc_timeseries: calculate a Sentinel timeseries for a file with polygons
#   - calc_maker: calculate a marker
#   - calc_periodic_mosaic: calculate periodic mosai
action = MUST_OVERRIDE

# Extra config files to load, in addition to general defaults.
# Possibilities to specify them:
#   - Absolute paths
#   - Relative paths: will be resolved towards the `default_basedir` or to the parent of
#     the tasks directory
#   - The {task_path} placeholder will be replaced by the current task file's path
#   - The {tasks_dir} placeholder will be replaced by the current task file's directory
extra_config_files_to_load = MUST_OVERRIDE

# The calc_marker_params section contains information to run a calc_marker task.
[calc_timeseries_params]

# input files
input_parcel_filename = MUST_OVERRIDE
# if you wandt to do a test run, set to True
test = False

# The `calc_marker_params` section contains information for this specific `calc_marker`
# task. Permanent configuration of the marker can be found in e.g. the `marker` section.
[calc_marker_params]

# The filename of the vector (polygon) geofile
input_parcel_filename = MUST_OVERRIDE
# The type of the input file (= which preprocessing is needed/can be done)  
input_parcel_filetype = MUST_OVERRIDE
# The lookup table (LUT) file where classes to classify to are specified
classes_refe_filename = MUST_OVERRIDE
# A groundtruth file for extra reporting, optional
input_groundtruth_filename
# The model to use if you want to reuse one instead of training one, optional
input_model_to_use_relativepath
# Do you want to reuse the last run dir for this marker run
reuse_last_run_dir = False
# Do you want to reuse the config used in the run dir you want to reuse
reuse_last_run_dir_config = False

# The calc_periodic_mosaic_params section contains information to run a calc_marker task
[calc_periodic_mosaic_params]

# Simulate, nothing is being downloaded
simulate = False

# The general section contains some general configuration ;-).
[general]

# File format (extension) to use for structured (table) data
data_ext = .sqlite
# File format (extension) to use for data that is output data (should be easy to use)
output_ext = .tsv
# File format for intermediary geo files (remark: gee needs .shp as input)
geofile_ext = .gpkg

# The log level to use
log_level = INFO

# The number of parallel threads/processes to start to do local processing. If -1, the
# number of available CPUs.
nb_parallel = -1

# Configuration regarding the region of interest (roi) we want to use.
[roi]

# The bounds of the roi to use to prepare images,...
roi_bounds = MUST_OVERRIDE
# The crs the roi bounds are specified in
roi_crs = MUST_OVERRIDE
# The name of the roi
roi_name = MUST_OVERRIDE

# Configuration of the period we want to use.
[period]

# Year to use. Should be overridden when used.
year = MUST_OVERRIDE

# Depending on the `period_name` specified in the ImageProfiles for the images to be
# generated/used, the `start_date` and `end_date` will be adjusted:
#   - "weekly": image periods will start on a monday and end on a sunday. If the
#     `start_date` is no monday, the first monday following it will be used. If the
#     `end_date` (exclusive) is no monday, the first monday before it will be used.
#   - "biweekly": image periods will start on even mondays of the year. If the
#      `start_date` is no monday, the first even monday following it will be used.
#      If the `end_date` (exclusive) is no monday, the first even monday before it
#      will be used.

# The start date of the period we want to use. The period starts at 00h of this day.
start_date = MUST_OVERRIDE
# The end date of the period we want to use.
# The period ends at 23h59 of the PREVIOUS day, so the end_date is exclusive. Supports
# placeholder {now}, to use the current date.
# Note that `end_date` will be overridden by (date.today() - `images_available_delay`)
# if that date is sooner than `end_date`.
end_date = MUST_OVERRIDE

# The number of days it can take for images to become available.
# If specified, the end date will be calculated as the minimum of `end_date` and
# (date.today() - `images_available_delay`). This avoids mosaics being generated before
# all images needed for them are available.
images_available_delay = 3

# Configuration of the images to be used.
[images]

# Images to get/use. This should basically be a list of existing image profile names as
# configured in imageprofile.ini.
# These are some examples of possible image profile names:
#   - s2-landcover: Sentinel 2 landcover, based on SCL mask
#   - s2-ndvi: Sentinel 2 ndvi
#   - s2-agri: Sentinel 2 bands relevant for agriculture (B02, B03, B04, B08, B11, B12)
#   - s1-grd-sigma0-asc: Sentinel 1 GRD, ascending orbit (VV, VH)
#   - s1-grd-sigma0-desc: Sentinel 1 GRD, descending orbit (VV, VH)
#   - s1-coh: Sentinel 1, 6 day or 12 day coherence, depending on availability (VV, VH)
#
# Optionally, it is possible to specify a subset of bands available in ImageProfile to
# be used e.g. in a marker. When a subset of bands is specified here this will be
# ignored for the purpose of loading image mosaics,... Image mosaics will always contain
# all bands defined in the image profile.
#   eg. [..., {"s2-grd-sigma0-asc": ["VV", "VH"]}]
images = MUST_OVERRIDE

# What to do if an image is missing:
#   - calculate_raise: calculate the image and raise an error if it fails. This is the default value.
#   - ignore: ignore that image, don't try to download it.
#   - calculate_ignore: calculate the image and ignore the error if it fails.
on_missing_image = calculate_raise

# Configuration on how/which periodic images/timeseries statistics should be generated.
[timeseries]

# Engine to use for the timeseries calculation.
# Possible values are pyqgis, rasterstats and exactextract.
engine = exactextract

# Stats to calculate for the timeseries. The following stats are available:
stats = [
             "count(min_coverage_frac=0.8,coverage_weight=none)",
             "mean(min_coverage_frac=0.8,coverage_weight=none)",
             "median(min_coverage_frac=0.8,coverage_weight=none)",
             "stdev(min_coverage_frac=0.8,coverage_weight=none)",
             "min(min_coverage_frac=0.8,coverage_weight=none)",
             "max(min_coverage_frac=0.8,coverage_weight=none)"
        ]
# Negative buffer to apply to input parcels to account for mixels.
buffer = 0

# The maximum percentage cloudcover an (S2) image can have to be used.
max_cloudcover_pct = 15

# The min percentage of parcels that need to have valid data for a time+sensor to use it 
min_parcels_with_data_pct = 80

# Configuration specific to the marker being calculated.
[marker]

# markertype, must be overriden in child ini files.
markertype = MUST_OVERRIDE

# minimum number of pixels that should be inside the (buffered) input parcels.
min_nb_pixels = -2 
# minimum number of pixels that should be inside the (buffered) input parcels used when
# training.
min_nb_pixels_train = ${marker:min_nb_pixels}

# classes that will be ignored for training + won't receive a prediction
classes_to_ignore = ${classes_to_ignore_default}
# define default ignores here, so it is easy to ADD extra for specific markers
classes_to_ignore_default = IGNORE:DIFFICULT_PERMANENT_CLASS, IGNORE:UNIMPORTANT, IGNORE:NOT_ENOUGH_SAMPLES, IGNORE:EARLY_CROP, IGNORE:LATE_CROP, IGNORE:LATE_MAINCROP, IGNORE:NEW_GRASSLAND, IGNORE:NOT_DECLARED

# classes that should be ignored for training, but have to get a prediction
classes_to_ignore_for_train = ${classes_to_ignore_for_train_default}
# define default ignores here, so it is easy to ADD extra for specific markers
classes_to_ignore_for_train_default = UNKNOWN

# classes that should specified as unimportant in the reporting
# Remark: this doesn't influence the training or predicting, these need to be set in 
# the other paremeters!
classes_to_ignore_unimportant = ${classes_to_ignore_unimportant_default}
# define default ignores here, so it is easy to ADD extra for specific markers
classes_to_ignore_unimportant_default = IGNORE:UNIMPORTANT, IGNORE:LATE_MAINCROP, IGNORE:NOT_DECLARED

# strategy to balance the training dataset for the marker. Possible values:
#   * BALANCING_STRATEGY_NONE: don't apply any balancing: 20% of the input samples per class is used for training
#   * BALANCING_STRATEGY_MEDIUM: 80% of input data is used for training, with maximum 10.000 samples per class and minimum 1.000 (samples will be duplicated if needed)
#   * BALANCING_STRATEGY_MEDIUM2: 80% of input data is used for training, with maximum 10.000 samples per class depending on input count + minimum 1.000 (samples will be duplicated if needed)
#   * BALANCING_STRATEGY_UPPER_LIMIT: 80% of input data is used for training, with a maximum of 10.000 samples per class
#   * BALANCING_STRATEGY_PROPORTIONAL_GROUPS: 80% of input data is used for training, but for classes with > 10.000 samples +- only half of those are used  
#   * BALANCING_STRATEGY_EQUAL: for each input class, the same amount of samples is used as training. For classes with few samples, (samples will be duplicated if needed)
balancing_strategy = BALANCING_STRATEGY_MEDIUM2

# The aggregation type to use on parcel level. Following options are avalable:
#    - mean: the mean of the pixel values in a parcel
#    - median: the median of the pixel values in a parcel
#    - std: the standard deviation of the pixel values in a parcel
parceldata_aggregations_to_use = median

# Postprocess...
postprocess_to_groups = 

# Configuration of marker parameters regarding the preprocessing of input files.
[preprocess]

# 
dedicated_data_columns = ${columns:id}, ${columns:class}, ${columns:class}_orig, ${columns:class_declared}, ${columns:class_declared}_orig, ${columns:pixcount_s1s2}
extra_export_columns = CODE_OBJ, LAYER_ID, PRC_ID, VERSIENR, ${columns:main_crop}
columns_to_keep = ${columns:is_early_late}, GESP_PM, GRAF_OPP, STAT_BGV, ${columns:main_crop}, ${columns:main_crop_declared}, ${columns:crop_gt_verified}, ${columns:crop_gt_unverified}, ${columns:late_crop}
# The way the classtype needs to be prepared
classtype_to_prepare = ${marker:markertype}
# The way the classtype for groundtruth needs to be prepared
classtype_to_prepare_groundtruth = ${classtype_to_prepare}-GROUNDTRUTH
# min amount of parcels that need to be in a specific class or it will get moved into IGNORE:NOT_ENOUGH_SAMPLES
min_parcels_in_class = 50

# Configuration of marker classifier parameters.
[classifier]

# The number of cross-prediction-models to use for this marker.
#
# If cross_pred_models is 0 (the default), no cross-prediction models are used.
#
# The cross-prediction-models feature can be used to avoid predicting a marker on a
# parcel where the parcel was part of the training dataset.
# When cross_pred_models = 2, the input data is split in 2 (per class). Each 50% of
# the data will be used to train one model, and this model will be used to
# predict the other 50% of the parcels.
cross_pred_models = 0

# The classifier type to use. Currently supported types:
#     * histgradientboostingclassifier (sklearn)
#     * keras_multilayer_perceptron (tensorflow)
#     * multilayer_perceptron (sklearn)
#     * nearestneighbour (sklearn)
#     * randomforest (sklearn)
#     * svm (sklearn)
classifier_type = keras_multilayer_perceptron

# For all sklearn based classifiers, any kwargs supported by the classifier chosen can
# be specified as a json parameter, e.g. for randomforest:
# classifier_sklearn_kwargs = {
#         "n_estimators": 200,
#         "max_depth": 35,
#         "min_samples_split": 10
#     }
classifier_sklearn_kwargs

# The fraction of the training data to be used to evaluate the model performance.
# Float between 0 and 1.
#
# Use depends on some factors:
#   - For keras classifiers, this will be used as validation data: it will not be
#     trained on, but it will be used to evaluate the loss and any model metrics on this
#     data at the end of each epoch.
#   - For sklearn classifiers, this will only be used during reporting.
#   - If cross_pred_models > 1, and an sklearn classifier is used, this parameter is
#     ignored as the predictions will happen on 100% unused data anyway, so the model
#     performance can be evaluated safely on the end result.
test_size = 0.2

# The extension of the file format to save the trained model to
classifier_ext = .hdf5

# For keras_multilayer_perceptron, hidden layer size(s) as a komma seperated list
# The {input_layer_size} can be used as placeholder and will be replaced by the number
# of input features (columns) that will be used. Using simple math is also supported.
multilayer_perceptron_hidden_layer_sizes = {input_layer_size}*2, {input_layer_size}*2, {input_layer_size}
# For keras_multilayer_perceptron: percentage dropout to use between the hidden layers
multilayer_perceptron_dropout_pct = 10
# For keras_multilayer_perceptron: the max number of iterations
# when training
multilayer_perceptron_max_iter = 1000
multilayer_perceptron_learning_rate_init = 0.001
# For keras, the strategy to choose the best model. Options:
#   - VAL_LOSS: choose the model with the lowest validation loss
#   - LOSS: choose the model with the lowest train loss
#   - AVG(VAL_LOSS,LOSS): choose the model with the lowest average of validation and
#     training loss
best_model_strategy = VAL_LOSS

# Configuration of marker postprocessing parameters.
[postprocess]

# The number of top classes to retain per parcel in the result.
top_classes = 3

# Doubt: if highest probability < 2*second probability  
doubt_proba1_st_2_x_proba2 = MUST_OVERRIDE
# Doubt: if prediction == input class and highest probability < thresshold
# Remark: should be floating point number from 0 till 1  
doubt_pred_eq_input_proba1_st_thresshold = MUST_OVERRIDE
# Doubt: if prediction != input class and highest probability < thresshold  
# Remark: should be floating point number from 0 till 1
doubt_pred_ne_input_proba1_st_thresshold = MUST_OVERRIDE

# Configuration of columns names to use from the input files for marker calculation.
[columns]

# The columns to include in the final output file
output_columns = LAYER_ID, PRC_ID, VERSIENR, markercode, ${class}, ${prediction_full_alpha}, ${prediction_cons_status}, cons_date, pred1, pred1_prob, pred2, pred2_prob, pred3, pred3_prob, modify_date

# Column name of the id column
id = UID
# Column name of the geom column
geom = geometry

# Column name of the crop on the parcel. For parcels that received an on the 
# spot check (already), this column contains the corrected data.
crop = GWSCOD_H

is_early_late = MON_VROEG_LAAT
early_crop = GWSCOD_V
main_crop = GWSCOD_H
main_crop_declared = GWSCOD_H_A
late_crop = GWSCOD_N
late_crop2 = GWSCOD_N2
# Column name of the crop on the parcel as declared by the farmer, even for
# parcels that already received an on the spot check.
crop_declared = GWSCOD_H_A
# Column name of the verified crop ground truth
crop_gt_verified = HOOFDTEELT_CTRL_COD
# Column name of the unverified crop ground truth
crop_gt_unverified = HOOFDTEELT_CTRL_COD_ORIG

# Column name of the classes to prepropress to in the refe file
class_refe = classname_refe
# Column name of the class, after preprocessing to optimize the classification
# For parcels that received an on the spot check, the class contains the 
# verified data so the training input is optimal
class = classname
# Column name of the class as declared by the farmer, even if the parcels that
# (already) received an on the spot check
class_declared = classname_declared
# Column name of a potential 2nd class as declared by the farmer, even if the parcels that
# (already) received an on the spot check
class_declared2 = classname_declared2
# Column name of the class to use for balancing the training dataset
class_balancing = ${class}
# Column name of the class of the verified groundtruth
class_groundtruth = classname_gt
# Column name of the count of the number of pixels for sentinel1/2 images
pixcount_s1s2 = pixcount
# Column name of the consolidated prediction: can be doubt, not_enough_pixels,...
prediction_cons = pred_consolidated
# Column name of the status of the consolidated prediction: can be OK, NOK
prediction_cons_status = pred_cons_status
# Column name of the status of the prediction to use for the alpa error
prediction_full_alpha = pred_full_alpha
# Column name of the detailed conclusion based on standard prediction
prediction_conclusion_detail = pred_conclusion_detail
# Column name of the detailed conclusion based on prediction with doubt
prediction_conclusion_detail_withdoubt = pred_conclusion_detail_withdoubt
# Column name of the detailed conclusion based on consolidated prediction
prediction_conclusion_detail_cons = pred_conclusion_detail_cons
# Column name of the detailed conclusion based on full alpha prediction
prediction_conclusion_detail_full_alpha = pred_conclusion_detail_full_alpha
# Column name of the conclusion based on consolidated prediction
prediction_conclusion_cons = pred_conclusion_cons

# Column name with the cross model index.
cross_pred_model_id = cross_pred_model_id

# Configuration of paths to use to find/save input/output files.
[paths]

# data_dir can be specified as a relative path which will be resolved to
# the parent dir of the tasks dir 
data_dir = .
# temp_dir can contain placeholder {tmp_dir}: resolves to the default system tempdir
temp_dir = {tmp_dir}/cropclassification
# marker_basedir can be specified as a relative path which will be resolved to
# the parent dir of the tasks dir
marker_basedir = .
marker_dir = ${paths:marker_basedir}/${period:year}_${marker:markertype}
images_periodic_dir = MUST_OVERRIDE
timeseries_periodic_dir = ${paths:data_dir}/_ts_periodic
timeseries_per_image_dir = ${paths:data_dir}/_ts_per_image
cover_periodic_dir = ${paths:data_dir}/_cover_periodic
input_dir = ${paths:data_dir}/_inputdata
input_preprocessed_dir = ${paths:data_dir}/_inputdata_preprocessed
model_dir = ${paths:data_dir}/_models
refe_dir = ${paths:data_dir}/_refe
log_dir = ${paths:marker_basedir}/log
gee = users/pieter_roggemans/

# Path to the image profiles
image_profiles_config_filepath = ${paths:marker_basedir}/_config/image_profiles.ini