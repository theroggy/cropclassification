# This is a config file with the shared/default settings for all markers. 

# The job section contains information about the (cropclassification) task you want to run 
[task]

# The action you want to run:
#    - calc_timeseries: calculate a Sentinel timeseries for a file with polygons
#    - calc_maker: calculate a marker
action = MUST_OVERRIDE

# Extra config files to load, in addition to general defaults
extra_config_files_to_load = MUST_OVERRIDE

# The calc_marker_params section contains information to run a calc_marker task
[calc_timeseries_params]

# input files
input_parcel_filename = MUST_OVERRIDE
# if you wandt to do a test run, set to True
test = False

# The calc_marker_params section contains information to run a calc_marker task
[calc_marker_params]

# What to do if an image is missing:
#   - ignore: ignore that the image, don't try to download it
#   - calculate_raise: calculate the image and raise an error if it fails
#   - calculate_ignore: calculate the image and ignore the error if it fails
on_missing_image = calculate_raise

# The type of the input file: typically country code
country_code = MUST_OVERRIDE
# The filename of the vector (polygon) geofile
input_parcel_filename = MUST_OVERRIDE
# The type of the input file (= which preprocessing is needed/can be done)  
input_parcel_filetype = ${country_code}
# The lookup table (LUT) file where classes to classify to are specified
classes_refe_filename = MUST_OVERRIDE
# A groundtruth file for extra reporting, optional
input_groundtruth_filename
# The model to use if you want to reuse one instead of training one, optional
input_model_to_use_relativepath
# Do you want to reuse the last run dir for this marker run
reuse_last_run_dir = False
# Do you want to reuse the config used in the run dir you want to reuse
reuse_last_run_dir_config = False

# The calc_periodic_mosaic_params section contains information to run a calc_marker task
[calc_periodic_mosaic_params]

# Depending on the period_name specified in the ImageProfiles, the `start_date` and
# `end_date` will be adjusted:
#   - "weekly": image periods will start on a monday and end on a sunday. If the
#     `start_date` is no monday, the first monday following it will be used. If the
#     `end_date` (exclusive) is no monday, the first monday before it will be used.
#   - "biweekly": image periods will start on even mondays of the year. If the
#      `start_date` is no monday, the first even monday following it will be used.
#      If the `end_date` (exclusive) is no monday, the first even monday before it
#      will be used.
# The start date of the period we want to download the mosaic
start_date_str = MUST_OVERRIDE
# The end date of the period we want to download the mosaic
# Also supports placeholder {now}, to use the current date
end_date_str = MUST_OVERRIDE
# The days before the end_date
# If not None, subtract end_date_subtract_days from end_date
end_date_subtract_days = 3
# The path to download the mosaics
dest_image_data_dir = //dg3.be/alp/Datagis/satellite_periodic/BEFL
# The image profiles to get
imageprofiles_to_get = s2-agri-weekly
# The CRS of the roi
roi_crs = 31370
# Simulate, nothing is being downloaded
simulate = False

# The general section contains some general confiuration ;-)
[general]

# File format (extension) to use for structured (table) data
data_ext = .sqlite
# File format (extension) to use for data that is output data (should be easy to use)
output_ext = .tsv
# File format for intermediary geo files (remark: gee needs .shp as input)
geofile_ext = .gpkg

# The log level to use
log_level = INFO

# The number of parallel threads/processes to start to do local processing. If -1, the
# number of available CPUs.
nb_parallel = -1

[marker]
# markertype, must be overriden in child ini files
markertype = MUST_OVERRIDE

# Year to use, should be overridden when running
year = MUST_OVERRIDE

# The bounds of the roi to use to prepare images,...
roi_bounds = MUST_OVERRIDE
# The crs the roi bounds are specified in
roi_crs = MUST_OVERRIDE
# The name of the roi
roi_name = MUST_OVERRIDE


# For more information about how the start and end dates map to image periodes, check
# the calc_periodic_mosaic_params section.
# Remark: year will be replace in run-time
# start date of timeseries data to use
start_date_str = ${year}-03-27 
# end date of timeseries data to use, NOT inclusive
end_date_str = ${year}-08-10

# negative buffer to apply to input parcels
buffer = 5
# minimum number of pixels that should be inside the buffered input parcels
min_nb_pixels = -2 
# minimum number of pixels that should be inside the buffered input parcels used when
# training
min_nb_pixels_train = ${marker:min_nb_pixels}

# classes that will be ignored for training + won't receive a prediction
classes_to_ignore = ${classes_to_ignore_default}
# define default ignores here, so it is easy to ADD extra for specific markers
classes_to_ignore_default = IGNORE_DIFFICULT_PERMANENT_CLASS, IGNORE_UNIMPORTANT, IGNORE_NOT_ENOUGH_SAMPLES, IGNORE_EARLY_CROP, IGNORE_LATE_CROP, IGNORE_LATE_MAINCROP, IGNORE_NEW_GRASSLAND, IGNORE_NOT_DECLARED

# classes that should be ignored for training, but have to get a prediction
classes_to_ignore_for_train = ${classes_to_ignore_for_train_default}
# define default ignores here, so it is easy to ADD extra for specific markers
classes_to_ignore_for_train_default = UNKNOWN

# classes that should specified as unimportant in the reporting
# Remark: this doesn't influence the training or predicting, these need to be set in 
# the other paremeters!
classes_to_ignore_unimportant = ${classes_to_ignore_unimportant_default}
# define default ignores here, so it is easy to ADD extra for specific markers
classes_to_ignore_unimportant_default = IGNORE_UNIMPORTANT, IGNORE_LATE_MAINCROP, IGNORE_NOT_DECLARED

# strategy to balance the training dataset for the marker. Possible values:
#   * BALANCING_STRATEGY_NONE: don't apply any balancing: 20% of the input samples per class is used for training
#   * BALANCING_STRATEGY_MEDIUM: 80% of input data is used for training, with maximum 10.000 samples per class and minimum 1.000 (samples will be duplicated if needed)
#   * BALANCING_STRATEGY_MEDIUM2: 80% of input data is used for training, with maximum 10.000 samples per class depending on input count + minimum 1.000 (samples will be duplicated if needed)
#   * BALANCING_STRATEGY_UPPER_LIMIT: 80% of input data is used for training, with a maximum of 10.000 samples per class
#   * BALANCING_STRATEGY_PROPORTIONAL_GROUPS: 80% of input data is used for training, but for classes with > 10.000 samples +- only half of those are used  
#   * BALANCING_STRATEGY_EQUAL: for each input class, the same amount of samples is used as training. For classes with few samples, (samples will be duplicated if needed)
balancing_strategy = BALANCING_STRATEGY_MEDIUM2

# Sensordata to use for the markers. This is a list of ImageProfile names:
#   - S1dB: Sentinel 1 data, in dB
#   - S1ascDesc: Sentinel 1 data, divided in Ascending and Descending passes
#   - S1dBAscDesc: Sentinel 1 data, in dB, divided in Ascending and Descending passes
#   - S1Coh: Sentinel 1 coherence
#   - S2gt95: Sentinel 2 data (B2,B3,B4,B8) IF available for 95% or area
#
#   - s2-landcover: Sentinel 2 landcover, based on SCL mask
#   - s2-ndvi: Sentinel 2 ndvi
#   - s2-agri: Sentinel 2 bands relevant for agriculture (B02, B03, B04, B08, B11, B12)
#   - s1-grd-sigma0-asc: Sentinel 1 GRD, ascending orbit (VV, VH)
#   - s1-grd-sigma0-desc: Sentinel 1 GRD, descending orbit (VV, VH)
#   - s1-coh: Sentinel 1, 6 day or 12 day coherence, depending on availability (VV, VH)
#
# Optionally, it is possible to specify the bands of the ImageProfile to be used like this:
#   eg. [..., {"s2-grd-sigma0-asc": ["VV", "VH"]}]
sensordata_to_use = ["s2-agri", {"s1-grd-sigma0-asc": ["VV", "VH"]}, {"s1-grd-sigma0-desc": ["VV", "VH"]}]

# The aggregation type to use on parcel level. Following options are avalable:
#    - mean: the mean of the pixel values in a parcel
#    - median: the median of the pixel values in a parcel
#    - std: the standard deviation of the pixel values in a parcel
parceldata_aggregations_to_use = median

# Postprocess...
postprocess_to_groups = 

# Path to the image profiles
image_profiles_config_filepath = ${dirs:marker_basedir}/_config/image_profiles.ini

[timeseries]
# The maximum percentage cloudcover an (S2) image can have to be used
max_cloudcover_pct = 15

# The min percentage of parcels that need to have valid data for a time+sensor to use it 
min_parcels_with_data_pct = 80

[preprocess]
# 
dedicated_data_columns = ${columns:id}, ${columns:class}, ${columns:class}_orig, ${columns:class_declared}, ${columns:class_declared}_orig, ${columns:pixcount_s1s2}
extra_export_columns = CODE_OBJ, LAYER_ID, PRC_ID, VERSIENR, GWSCOD_H
columns_to_keep = MON_EARLY_LATE, GESP_PM, GRAF_OPP, STAT_BGV, GWSCOD_H, GWSCOD_H_A, HOOFDTEELT_CTRL_COD, HOOFDTEELT_CTRL_COD_ORIG, GWSCOD_N
# The way the classtype needs to be prepared
classtype_to_prepare = ${marker:markertype}
# The way the classtype for groundtruth needs to be prepared
classtype_to_prepare_groundtruth = ${classtype_to_prepare}-GROUNDTRUTH
# min amount of parcels that need to be in a specific class or it will get moved into IGNORE_NOT_ENOUGH_SAMPLES
min_parcels_in_class = 50

[classifier]
# The classifier type to use. Currently supported types:
#     * histgradientboostingclassifier (sklearn)
#     * keras_multilayer_perceptron (tensorflow)
#     * multilayer_perceptron (sklearn)
#     * nearestneighbour (sklearn)
#     * randomforest (sklearn)
#     * svm (sklearn)
classifier_type = keras_multilayer_perceptron

# For all sklearn based classifiers, any kwargs supported by the classifier chosen can
# be specified as a json parameter, e.g. for randomforest:
# classifier_sklearn_kwargs = {
#         "n_estimators": 200,
#         "max_depth": 35,
#         "min_samples_split": 10
#     }
classifier_sklearn_kwargs

# The extension of the file format to save the trained model to
classifier_ext = .hdf5

# For keras_multilayer_perceptron, hidden layer size(s) as a komma seperated list
# The {input_layer_size} can be used as placeholder and will be replaced by the number
# of input features (columns) that will be used. Using simple math is also supported.
multilayer_perceptron_hidden_layer_sizes = {input_layer_size}*2, {input_layer_size}*2, {input_layer_size}
# For keras_multilayer_perceptron: percentage dropout to use between the hidden layers
multilayer_perceptron_dropout_pct = 10
# For keras_multilayer_perceptron: the max number of iterations
# when training
multilayer_perceptron_max_iter = 1000
multilayer_perceptron_learning_rate_init = 0.001
# For keras, the strategy to choose the best model. Options:
#   - VAL_LOSS: choose the model with the lowest validation loss
#   - LOSS: choose the model with the lowest train loss
#   - AVG(VAL_LOSS,LOSS): choose the model with the lowest average of validation and
#     training loss
best_model_strategy = VAL_LOSS

[postprocess]
# Doubt: if highest probability < 2*second probability  
doubt_proba1_st_2_x_proba2 = MUST_OVERRIDE
# Doubt: if prediction == input class and highest probability < thresshold
# Remark: should be floating point number from 0 till 1  
doubt_pred_eq_input_proba1_st_thresshold = MUST_OVERRIDE
# Doubt: if prediction != input class and highest probability < thresshold  
# Remark: should be floating point number from 0 till 1
doubt_pred_ne_input_proba1_st_thresshold = MUST_OVERRIDE

[columns]
# The columns to include in the final output file
output_columns = LAYER_ID, PRC_ID, VERSIENR, markercode, run_id, ${class}, ${prediction_full_alpha}, ${prediction_cons_status}, cons_date, pred1, pred1_prob, pred2, pred2_prob, pred3, pred3_prob, modify_date

# Column name of the id column
id = UID
# Column name of the geom column
geom = geometry

# Column name of the crop on the parcel. For parcels that received an on the 
# spot check (already), this column contains the corrected data.
crop = GWSCOD_H

early_crop = GWSCOD_V
main_crop = GWSCOD_H
main_crop_declared = GWSCOD_H_A
late_crop = GWSCOD_N
late_crop2 = GWSCOD_N2
# Column name of the crop on the parcel as declared by the farmer, even for
# parcels that already received an on the spot check.
crop_declared = GWSCOD_H_A
# Column name of the verified crop ground truth
crop_gt_verified = HOOFDTEELT_CTRL_COD
# Column name of the unverified crop ground truth
crop_gt_unverified = HOOFDTEELT_CTRL_COD_ORIG

# Column name of the classes to prepropress to in the refe file
class_refe = classname_refe
# Column name of the class, after preprocessing to optimize the classification
# For parcels that received an on the spot check, the class contains the 
# verified data so the training input is optimal
class = classname
# Column name of the class as declared by the farmer, even if the parcels that
# (already) received an on the spot check
class_declared = classname_declared
# Column name of a potential 2nd class as declared by the farmer, even if the parcels that
# (already) received an on the spot check
class_declared2 = classname_declared2
# Column name of the class to use for balancing the training dataset
class_balancing = ${class}
# Column name of the class of the verified groundtruth
class_groundtruth = classname_gt
# Column name of the count of the number of pixels for sentinel1/2 images
pixcount_s1s2 = pixcount
# Column name of the consolidated prediction: can be doubt, not_enough_pixels,...
prediction_cons = pred_consolidated
# Column name of the status of the consolidated prediction: can be OK, NOK
prediction_cons_status = pred_cons_status
# Column name of the status of the prediction to use for the alpa error
prediction_full_alpha = pred_full_alpha
# Column name of the detailed conclusion based on standard prediction
prediction_conclusion_detail = pred_conclusion_detail
# Column name of the detailed conclusion based on prediction with doubt
prediction_conclusion_detail_withdoubt = pred_conclusion_detail_withdoubt
# Column name of the detailed conclusion based on consolidated prediction
prediction_conclusion_detail_cons = pred_conclusion_detail_cons
# Column name of the detailed conclusion based on full alpha prediction
prediction_conclusion_detail_full_alpha = pred_conclusion_detail_full_alpha
# Column name of the conclusion based on consolidated prediction
prediction_conclusion_cons = pred_conclusion_cons

[dirs]
# Directories to use
# data_dir can be specified as a relative path which will be resolved to
# the parent dir of the tasks dir 
data_dir = .
# temp_dir can contain placeholder {tmp_dir}: resolves to the default system tempdir
temp_dir = {tmp_dir}/cropclassification
# marker_basedir can be specified as a relative path which will be resolved to
# the parent dir of the tasks dir
marker_basedir = .
marker_dir = ${dirs:marker_basedir}/${marker:year}_${marker:markertype}
images_periodic_dir = ${dirs:data_dir}/_images_periodic/${marker:roi_name}
timeseries_periodic_dir = ${dirs:data_dir}/_ts_periodic
timeseries_per_image_dir = ${dirs:data_dir}/_ts_per_image
input_dir = ${dirs:data_dir}/_inputdata
input_preprocessed_dir = ${dirs:data_dir}/_inputdata_preprocessed
model_dir = ${dirs:data_dir}/_models
refe_dir = ${dirs:data_dir}/_refe
log_dir = ${dirs:marker_basedir}/log
gee = users/pieter_roggemans/
