# This is a config file with the shared/default settings for all markers. 

# The job section contains information about the (cropclassification) task you want to run 
[task]

# The action you want to run:
#    - calc_timeseries: calculate a Sentinel timeseries for a file with polygons
#    - calc_maker: calculate a marker
action = MUST_OVERRIDE

# Extra config files to load, in addition to general defaults
extra_config_files_to_load = MUST_OVERRIDE

# The calc_marker_params section contains information to run a calc_marker task
[calc_timeseries_params]

# input files
input_parcel_filename = MUST_OVERRIDE
# if you wandt to do a test run, set to True
test = False

# The calc_marker_params section contains information to run a calc_marker task
[calc_marker_params]

# The type of the input file: typically country code
country_code = BEFL
# The filename of the vector (polygon) geofile
input_parcel_filename = MUST_OVERRIDE
# The type of the input file (= which preprocessing is needed/can be done)  
input_parcel_filetype = ${country_code}
# The lookup table (LUT) file where classes to classify to are specified
classes_refe_filename = MUST_OVERRIDE
# A groundtruth file for extra reporting, optional
input_groundtruth_filename
# The model to use if you want to reuse one instead of training one, optional
input_model_to_use_relativepath
# Do you want to reuse the last run dir for this marker run
reuse_last_run_dir = False
# Do you want to reuse the config used in the run dir you want to reuse
reuse_last_run_dir_config = False

# The general section contains some general confiuration ;-)
[general]

# File format (extension) to use for structured (table) data
data_ext = .sqlite
# File format (extension) to use for data that is output data (should be easy to use)
output_ext = .tsv
# File format for intermediary geo files (remark: gee needs .shp as input)
geofile_ext = .gpkg

# The log level to use
log_level = INFO

[marker]
# markertype, must be overriden in child ini files
markertype = MUST_OVERRIDE

# Year to use, should be overridden when running
year = MUST_OVERRIDE

# start date of timeseries data to use
# remarks: nearest monday will be used + year will be replace in run-time
start_date_str = ${year}-03-27 
# end date of timeseries data to use
# remarks: end date is NOT inclusive + year will be replace in run-time
end_date_str = ${year}-08-10
# negative buffer to apply to input parcels
buffer = 5
# minimum number of pixels that should be inside the buffered input parcels
min_nb_pixels = -2 
# minimum number of pixels that should be inside the buffered input parcels used when training
min_nb_pixels_train = ${marker:min_nb_pixels}

# classes that will be ignored for training + won't receive a prediction
classes_to_ignore = ${classes_to_ignore_default}
# define default ignores here, so it is easy to ADD extra for specific markers
classes_to_ignore_default = IGNORE_DIFFICULT_PERMANENT_CLASS, IGNORE_UNIMPORTANT, IGNORE_NOT_ENOUGH_SAMPLES, IGNORE_EARLY_CROP, IGNORE_LATE_CROP, IGNORE_NEW_GRASSLAND

# classes that should be ignored for training, but have to get a prediction
classes_to_ignore_for_train = ${classes_to_ignore_for_train_default}
# define default ignores here, so it is easy to ADD extra for specific markers
classes_to_ignore_for_train_default = UNKNOWN

# classes that should specified as unimportant in the reporting
# Remark: this doesn't influence the training or predicting, these need to be set in 
# the other paremeters!
classes_to_ignore_unimportant = ${classes_to_ignore_unimportant_default}
# define default ignores here, so it is easy to ADD extra for specific markers
classes_to_ignore_unimportant_default = IGNORE_UNIMPORTANT

# strategy to balance the training dataset for the marker. Possible values:
#   * BALANCING_STRATEGY_NONE: don't apply any balancing: 20% of the input samples per class is used for training
#   * BALANCING_STRATEGY_MEDIUM: 80% of input data is used for training, with maximum 10.000 samples per class and minimum 1.000 (samples will be duplicated if needed)
#   * BALANCING_STRATEGY_MEDIUM2: 80% of input data is used for training, with maximum 10.000 samples per class depending on input count + minimum 1.000 (samples will be duplicated if needed)
#   * BALANCING_STRATEGY_UPPER_LIMIT: 80% of input data is used for training, with a maximum of 10.000 samples per class
#   * BALANCING_STRATEGY_PROPORTIONAL_GROUPS: 80% of input data is used for training, but for classes with > 10.000 samples +- only half of those are used  
#   * BALANCING_STRATEGY_EQUAL: for each input class, the same amount of samples is used as training. For classes with few samples, (samples will be duplicated if needed)
balancing_strategy = BALANCING_STRATEGY_MEDIUM2

# Sensordata to use for the markers. Options are the following:
#   - S1-dB: Sentinel 1 data, in dB
#   - S1-ascDesc: Sentinel 1 data, divided in Ascending and Descending passes
#   - S1-dBAscDesc: Sentinel 1 data, in dB, divided in Ascending and Descending passes
#   - S1-Coh: Sentinel 1 coherence
#   - S2-gt95: Sentinel 2 data (B2,B3,B4,B8) IF available for 95% or area
#   - S2-landcover: Sentinel 2 landcover, based on SCL mask
#   - S2-ndvi: Sentinel 2 ndvi
#   - S2-agri
sensordata_to_use = S2-agri

# The aggregation type to use on parcel level. Following options are avalable:
#    - mean: the mean of the pixel values in a parcel
#    - median: the median of the pixel values in a parcel
#    - std: the standard deviation of the pixel values in a parcel
parceldata_aggregations_to_use = median

# Postprocess...
postprocess_to_groups = 

[timeseries]
# The way timeseries are calculated
#     * gee: google earth engine
#     * onda: ONDA DIAS
timeseries_calc_type = onda

# The maximum percentage cloudcover an (S2) image can have to be used
max_cloudcover_pct = 15

# The min percentage of parcels that need to have valid data for a time+sensor to use it 
min_parcels_with_data_pct = 90

# Bands to extract for S2 images 
s2bands = B02-10m, B03-10m, B04-10m, B08-10m, B11-20m, B12-20m, SCL-20m

[preprocess]
# 
dedicated_data_columns = ${columns:id}, ${columns:class}, ${columns:class}_orig, ${columns:class_declared}, ${columns:class_declared}_orig, ${columns:pixcount_s1s2}
extra_export_columns = CODE_OBJ, LAYER_ID, PRC_ID, VERSIENR, GWSCOD_H

# The way the classtype needs to be prepared
classtype_to_prepare = ${marker:markertype}
# The way the classtype for groundtruth needs to be prepared
classtype_to_prepare_groundtruth = ${classtype_to_prepare}-GROUNDTRUTH

[classifier]
# The classifier type to use. Currently supported types:
#     * keras_multilayer_perceptron (using keras)
#     * multilayer_perceptron (using sklearn)
#     * nearestneighbour
#     * randomforest
#     * svm
classifier_type = keras_multilayer_perceptron
# The extension of the file format to save the trained model to
classifier_ext = .hdf5

# For multilayer_perceptron, the hidden layer size(s) (as a komma seperated list)
multilayer_perceptron_hidden_layer_sizes = 100, 100
# For multilayer_perceptron, the percentage dropout to use between the hidden layers
multilayer_perceptron_dropout_pct = 30
# For multilayer_perceptron, the maximum number of iterations when training
multilayer_perceptron_max_iter = 1000
multilayer_perceptron_learning_rate_init = 0.001

# For randomforest, the maximum number of trees to create
randomforest_n_estimators = 200
# For randomforest, the maximum depth of trees to create
randomforest_max_depth = 35

[postprocess]
# Doubt: if highest probability < 2*second probability  
doubt_proba1_st_2_x_proba2 = MUST_OVERRIDE
# Doubt: if prediction == input class and highest probability < thresshold
# Remark: should be floating point number from 0 till 1  
doubt_pred_eq_input_proba1_st_thresshold = MUST_OVERRIDE
# Doubt: if prediction != input class and highest probability < thresshold  
# Remark: should be floating point number from 0 till 1
doubt_pred_ne_input_proba1_st_thresshold = MUST_OVERRIDE

[columns]
# The columns to include in the final output file
output_columns = LAYER_ID, PRC_ID, VERSIENR, markercode, run_id, ${class}, ${prediction_full_alpha}, ${prediction_cons_status}, cons_date, pred1, pred1_prob, pred2, pred2_prob, pred3, pred3_prob, modify_date

# Column name of the id column
id = UID
# Column name of the geom column
geom = geometry

# Column name of the crop on the parcel. For parcels that received an on the 
# spot check (already), this column contains the corrected data.
crop = GWSCOD_H
# Column name of the crop on the parcel as declared by the farmer, even for
# parcels that already received an on the spot check.
crop_declared = GWSCOD_H_A
# Column name of the classes to prepropress to in the refe file
class_refe = classname_refe
# Column name of the class, after preprocessing to optimize the classification
# For parcels that received an on the spot check, the class contains the 
# verified data so the training input is optimal
class = classname
# Column name of the class as declared by the farmer, even if the parcels that
# (already) received an on the spot check
class_declared = classname_declared
# Column name of the class to use for balancing the training dataset
class_balancing = ${class}
# Column name of the class of the verified groundtruth
class_groundtruth = classname_gt
# Column name of the count of the number of pixels for sentinel1/2 images
pixcount_s1s2 = pixcount
# Column name of the consolidated prediction: can be doubt, not_enough_pixels,...
prediction_cons = pred_consolidated
# Column name of the status of the consolidated prediction: can be OK, NOK
prediction_cons_status = pred_cons_status
# Column name of the status of the prediction to use for the alpa error
prediction_full_alpha = pred_full_alpha
# Column name of the detailed conclusion based on standard prediction
prediction_conclusion_detail = pred_conclusion_detail
# Column name of the detailed conclusion based on prediction with doubt
prediction_conclusion_detail_withdoubt = pred_conclusion_detail_withdoubt
# Column name of the detailed conclusion based on consolidated prediction
prediction_conclusion_detail_cons = pred_conclusion_detail_cons
# Column name of the detailed conclusion based on full alpha prediction
prediction_conclusion_detail_full_alpha = pred_conclusion_detail_full_alpha
# Column name of the conclusion based on consolidated prediction
prediction_conclusion_cons = pred_conclusion_cons

[dirs]
# Directories to use
# data_dir can be specified as a relative path which will be resolved to
# the parent dir of the tasks dir 
data_dir = .
temp_dir = ${dirs:data_dir}/tmp
# marker_basedir can be specified as a relative path which will be resolved to
# the parent dir of the tasks dir
marker_basedir = .
marker_dir = ${dirs:marker_basedir}/${marker:year}_${marker:markertype}
images_periodic_dir = ${dirs:data_dir}/_images_periodic/${calc_marker_params:country_code}
timeseries_periodic_dir = ${dirs:data_dir}/_ts_periodic
timeseries_per_image_dir = ${dirs:data_dir}/_ts_per_image
input_dir = ${dirs:data_dir}/_inputdata
input_preprocessed_dir = ${dirs:data_dir}/_inputdata_preprocessed
model_dir = ${dirs:data_dir}/_models
refe_dir = ${dirs:data_dir}/_refe
log_dir = ${dirs:marker_basedir}/log
gee = users/pieter_roggemans/
